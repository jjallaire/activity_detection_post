---
title: "Using Keras to detect activity"
description: |
  Using accelerometer data recorded from a smart phone, use a 1-D convolutional neural network to predict the various physical activities being performed accross multiple individuals. 
author:
  - name: Nick Strayer 
    url: http://nickstrayer.me
    affiliation: Vanderbilt University
    affiliation_url: https://www.vanderbilt.edu/biostatistics-graduate/
date: "`r Sys.Date()`"
output: radix::radix_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Libraries

```{r}
library(keras)
library(tidyverse)
library(knitr)
library(ggridges) 
```

## The data

The data come from the [Smartphone-Based Recognition of Human Activities and Postural Transitions Data Set ](http://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions) distributed by the University of California, Irvine. 

The main datasource when downloaded from the link above contains two different 'parts' of the data. One that has been pre-processed using various feature extraction techniques such as fast fourier transform, and another `RawData` section that simply supplies the raw X,Y,Z directions of an accelerometer and gyroscope. This is the dataset we will use. 

__The labels__

The data has integer encodings for the activities which, while not important to the model itself, are helpful for use to see. Let's load them first. 

```{r}
activityLabels <- read.table('data/activity_labels.txt', col.names = c('number', 'label')) 

activityLabels %>% kable()
```

Next we will load in the labels key for the `RawData`. The key for the columns is taken from the data `README.txt`. 

```
Column 1: experiment number ID, 
Column 2: user number ID, 
Column 3: activity number ID 
Column 4: Label start point (in number of signal log samples (recorded at 50Hz))
Column 5: Label end point (in number of signal log samples)
```

```{r}
labels <- read.table(
  'data/RawData/labels.txt',
  col.names = c('experiment', 'userId', 'activity', 'startPos', 'endPos')
)

labels %>% 
  head() %>% 
  kable()
```

Next, let's look at the actual files of the user data provided to us in `RawData/`

```{r}
dataFiles <- list.files('data/RawData')

dataFiles %>% head()
```

So we have a three-part file naming scheme. The first is the type of data the file contains: either `acc` for accelerometer or `gyro` for gyroscope. Next is the experiment number, and last is the user Id for the recording. Let's load these into a dataframe for ease of use later. 

```{r}
fileInfo <- data_frame(
  filePath = dataFiles
) %>%
  filter(filePath != 'labels.txt') %>% 
  separate(filePath, sep = '_', into = c('type', 'experiment', 'userId'), remove = FALSE) %>% 
  mutate(
    experiment = str_remove(experiment, 'exp'),
    userId = str_remove_all(userId, 'user|\\.txt')
  ) %>% 
  spread(type, filePath)

fileInfo %>% head() %>% kable()
```

```{r, eval = FALSE}
# Read contents of single file to a dataframe with accelerometer and gyro data.
readInData <- function(experiment, userId){
  genFilePath = function(type){
    return(paste0('data/RawData/', type, '_exp',experiment, '_user', userId, '.txt'))
  }  
  
  bind_cols(
    read.table(genFilePath('acc'), col.names = c('a_x', 'a_y', 'a_z')),
    read.table(genFilePath('gyro'), col.names = c('g_x', 'g_y', 'g_z'))
  )
}

# Function to read a given file and get the observations contained along with their classes.
loadFileData <- function(curExperiment, curUserId){
  # load sensor data from file into dataframe
  allData <- readInData(curExperiment, curUserId)

  extractObservation <- function(startPos, endPos){
    allData[startPos:endPos,]
  }
  
  # get observation locations in this file from labels dataframe
  dataLabels <- labels %>% 
    filter(userId == as.integer(curUserId), experiment == as.integer(curExperiment))
  

  # extract observations as dataframes and save as a column in dataframe.
  dataLabels %>% 
    mutate(
      data = map2(startPos, endPos, extractObservation)
    ) %>% 
    select(-startPos, -endPos)
}

allObservations <- map2_df(fileInfo$experiment, fileInfo$userId, loadFileData) %>% 
  right_join(activityLabels, by = c('activity' = 'number')) %>% 
  rename(activityName = label)

write_rds(allObservations, 'allObservations.rds')
allObservations %>% dim()
```

```{r, echo = FALSE}
allObservations <- read_rds('allObservations.rds')
allObservations %>% dim()
```

So now that we have all the data loaded and our labels as either `experiment`, `userId`, or `activity` we can explore the dataset. 

Before we do so, however, we need to set aside a testing set of observations that we won't look at until after we have trained our model in an attempt to get the least-biased view of our performance. 

We will leave out 20% of the data for this purpose, making sure that no user is in both the train and testing data (in an effort see how generalizable our model is to new users.)



### Visualizing activities

Let's first look at the length of the recordings by activity. 

```{r, fig.height=6, fig.width=5}
allObservations %>% 
  mutate(recording_length = map_int(data,nrow)) %>% 
  ggplot(aes(x = recording_length, y = activityName)) +
  geom_density_ridges()
```

From this plot we can see that there appear to be three general activity length groups. THe first and largest group being of recording lengths between 0 and 500 time-points, the second and smallest, being the two stair walking activities with lengths between 500 and 750, and lastly the much wider distributed classes with an average a little over 1,000. 

This fact requires us to be a bit careful with how we proceed. If we train the model on every class at once we are going to have to pad all the observations to the length of the longest, which would leave a large majority of the observations with a huge proportion of their data being just padding-zeros. Because of this, we will fit our model to just the largest group of observations, these include `STAND_TO_SIT`, `STAND_TO_LIE`, `SIT_TO_STAND`, `SIT_TO_LIE`, `LIE_TO_STAND`, and `LIE_TO_SIT`. 

An interesting future direction would be attempting to use another architecture such as an RNN that can handle variable length inputs and training it on all the data. However, you would run the risk of the model learning simply that if the observation is long it is most likely one of the four longest classes which would not generalize to a scenario where you were running this model on a real-time-stream of data. 


__Filter activities__


Based on our work from above, let's subset the data to just be of the activities of interest. 


```{r}
desiredActivities <- c(
  "STAND_TO_SIT", "SIT_TO_STAND", "SIT_TO_LIE", 
  "LIE_TO_SIT", "STAND_TO_LIE", "LIE_TO_STAND"  
)

filteredObservations <- allObservations %>% 
  filter(activityName %in% desiredActivities) %>% 
  mutate(observationId = 1:n())

nrow(filteredObservations)
```

So after our aggressive pruning of the data we will have a respectable amount of data left upon which our model can learn. 

### Train-Test Split

Before we go any futher into exploring the data for our model, in an attempt to be as fair as possible with our performance measures we need to split the data into a train and test set. Since each user performed all activities just once (with the exception of one who only did 10 of the 12) by splitting on `userId` we will ensure that our model sees new people exclusively when it trains. 

```{r}
# get all users
userIds <- allObservations$userId %>% unique()

# randomly choose 24 (80% of 30) for training
set.seed(42) # seed for reproducability
trainIds <- sample(userIds, size = 24)

# set the rest of the users to the testing set
testIds <- setdiff(userIds,trainIds)

# filter data. 
trainData <- filteredObservations %>% 
  filter(userId %in% trainIds)

testData <- filteredObservations %>% 
  filter(userId %in% testIds)
```

__Visualizing each class__

Now that we have cut our data down, we can actually visualize the data for each class to see if there's any immediately descernable shape that our model may be able to pick up on. 

First let's unpack our data from its dataframe of one-row-per-observation to a tidy version of all the observations. 

```{r}
unpackedObs <- 1:nrow(trainData) %>% 
  map_df(function(rowNum){
    dataRow <- trainData[rowNum, ]
    dataRow$data[[1]] %>% 
      mutate(
        activityName = dataRow$activityName, 
        observationId = dataRow$observationId,
        time = 1:n() )
  }) %>% 
  gather(reading, value, -time, -activityName, -observationId) %>% 
  separate(reading, into = c('type', 'direction'), sep = '_') %>% 
  mutate(type = ifelse(type == 'a', 'acceleration', 'gyro'))
```

Now we have an unpacked set of our observations, let's visualize them!

```{r,layout="l-screen", fig.width=12, fig.height=4}
unpackedObs %>% 
  ggplot(aes(x = time, y = value, color = direction)) +
  geom_line(alpha = 0.2) +
  geom_smooth(se = FALSE, alpha = 0.7, size = 0.5) +
  facet_grid(type ~ activityName, scales = 'free_y') +
  theme_minimal() +
  theme(
    axis.text.x = element_blank()
  )
```

So at least in the accelerometer data patterns definitely emerge. One would imagine that the model may have trouble with the differences between `LIE_TO_SIT` and `LIE_TO_STAND` as they have a similar profile on average. The same goes for `SIT_TO_STAND` and `STAND_TO_SIT`. 


### Padding observations

First we will decide what length to pad (and truncate) our sequences to by finding what the 98th percentile length. This will help us avoid extra-long outlier recordings messing up the padding. 

```{r}
padSize <- trainData$data %>% 
  map_int(nrow) %>% 
  quantile(p = 0.98) %>% 
  ceiling()
padSize
```
Now we simply need to convert our list of observations to matrices, then use the super handy [`pad_sequences()`](https://keras.rstudio.com/reference/pad_sequences.html) function in Keras to pad all observations and turn them into a 3D tensor for us. 

```{r}
convertToTensor <- . %>% 
  map(as.matrix) %>% 
  pad_sequences(maxlen = padSize)

trainObs <- trainData$data %>% 
  convertToTensor()

testObs <- testData$data %>% 
  convertToTensor()
  
dim(trainObs)
```

Wonderful, we now have our data in a nice neural-network-friendly format of a 3D tensor with dimensions `(<num obs>, <sequence length>, <channels>)`. If we were working with a video instead of sensor data, this would be a 4D Tensor. If we were using FMRI data, this could be a 5D tensor!

__One-Hot (Dummy) encoding the classes__

There's one last thing we need to do before we can train our model, and that is turn our observation classes from integers to a one-hot, or dummy encoded, vectors. Luckily, again Keras has supplied us with a very helpful function to do just this. 

```{r}
oneHotClasses <- . %>% 
  {. - 7} %>%        # bring integers down to 0-6 from 7-12
  to_categorical() # One-hot encode

trainY <- trainData$activity %>% oneHotClasses()
testY <- testData$activity %>% oneHotClasses()
```


### Model architecture

```{r}
input_shape <- dim(trainObs)[-1]
num_classes <- dim(trainY)[2]

filters <- 24
kernel_size <- 8
dense_size <- 48

#Initialize model
model <- keras_model_sequential()
model %>% 
  layer_conv_1d(
    filters = filters,
    kernel_size = kernel_size, 
    input_shape = input_shape,
    padding = "valid", 
    activation = "relu",
    name = 'convolution'
  ) %>%
  layer_batch_normalization() %>%
  layer_spatial_dropout_1d(0.3) %>% 
  layer_conv_1d(
    filters = filters/2,
    kernel_size = kernel_size,
    activation = "relu",
  ) %>%
  # Apply max pooling:
  layer_global_average_pooling_1d() %>% 
  layer_batch_normalization() %>%
  layer_dropout(0.5) %>% 
  layer_dense(
    dense_size,
    activation = 'relu'
  ) %>% 
  layer_batch_normalization() %>%
  layer_dropout(0.4) %>% 
  layer_dense(
    num_classes, 
    activation = 'softmax',
    name = 'dense_output'
  ) 

summary(model)
```


### Training

```{r, eval = FALSE}
# Compile model
model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "rmsprop",
  metrics = "accuracy"
)

trainHistory <- model %>%
  fit(
    x = trainObs, y = trainY,
    epochs = 250,
    validation_data = list(testObs, testY),
    callbacks = list(
      callback_model_checkpoint("best_model.h5", save_best_only = TRUE)
    )
  )

saveRDS(trainHistory, 'trainHistory.rds') # cache
```

```{r, echo = FALSE}
trainHistory <- read_rds('trainHistory.rds')
```


__Plotting Training Results__

```{r, echo = FALSE, layout = 'l-page', fig.width = 6, fig.height = 3}
historyDf <- trainHistory %>% as.data.frame() 
best_results <- historyDf %>% 
  group_by(metric) %>% 
  summarise(
    low = min(value),
    lowEpoch = first(epoch[value == low]),
    high = max(value),
    highEpoch = first(epoch[value == high])
  ) %>% 
  mutate( 
    best = ifelse(metric == 'loss', low, high),
    bestEpoch = ifelse(metric == 'loss', lowEpoch, highEpoch),
    ypos = ifelse(metric == 'loss', 1.5, 0.4)
  ) 

historyDf %>% 
  ggplot(aes(x = epoch, y = value, color = data)) +
  geom_text(
    data = best_results, 
    aes(x = 175, y = ypos, label = paste('best:',round(best,3))),
    color = 'black', fill = 'black', hjust = 1, nudge_x = -2
  ) +
  geom_segment(
    data = best_results, 
    aes(x = 175, xend = bestEpoch, y = ypos, yend = best), 
    color = 'black', fill = 'black', alpha = 0.4
  ) +
  geom_line(size = 0.2) +
  geom_point(size = 0.4) +
  facet_grid(metric~., scales = 'free_y') +
  theme_minimal()
```


### Testing


### Investigating Misclassifications

### Future Directions






